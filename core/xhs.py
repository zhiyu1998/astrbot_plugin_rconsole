import re
import os
import json
import httpx
import asyncio
import aiohttp
import time
import random
from typing import Any, Dict, List, AsyncGenerator, Optional
from urllib.parse import urlparse, parse_qs

import astrbot.api.message_components as Comp
from astrbot.api.event import AstrMessageEvent
from astrbot.api import logger

from .common import delete_boring_characters, remove_files, send_forward_message

# Constants
XHS_REQ_LINK = "https://www.xiaohongshu.com/explore/"
COMMON_HEADER = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# æ•°æ®ç›®å½•å’Œç¼“å­˜ç›®å½•
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
CACHE_DIR = os.path.join(DATA_DIR, "xhs_cache")
TEMP_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "temp")

# ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
try:
    os.makedirs(CACHE_DIR, exist_ok=True)
    os.makedirs(TEMP_DIR, exist_ok=True)
    logger.info(f"ç¡®ä¿å°çº¢ä¹¦ç¼“å­˜ç›®å½•å­˜åœ¨: {CACHE_DIR}")
    logger.info(f"ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨: {TEMP_DIR}")
except Exception as e:
    logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥: {e}")

async def download_img(url: str, path: str, session: Optional[aiohttp.ClientSession] = None) -> str:
    """
    ä¸‹è½½å›¾ç‰‡åˆ°æŒ‡å®šè·¯å¾„
    
    :param url: å›¾ç‰‡é“¾æ¥
    :param path: ä¿å­˜è·¯å¾„
    :param session: å¯é€‰çš„ aiohttp ä¼šè¯
    :return: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    if session:
        async with session.get(url) as response:
            with open(path, 'wb') as fd:
                async for chunk in response.content.iter_chunked(1024):
                    fd.write(chunk)
    else:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                with open(path, 'wb') as fd:
                    async for chunk in response.content.iter_chunked(1024):
                        fd.write(chunk)
    return path

async def download_video(url: str) -> str:
    """
    ä¸‹è½½è§†é¢‘åˆ°ä¸´æ—¶è·¯å¾„
    
    :param url: è§†é¢‘é“¾æ¥
    :return: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
    os.makedirs(TEMP_DIR, exist_ok=True)
    
    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    filename = f"xhs_video_{int(time.time())}_{random.randint(1000, 9999)}.mp4"
    path = os.path.join(TEMP_DIR, filename)
    
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            with open(path, 'wb') as fd:
                async for chunk in response.content.iter_chunked(1024):
                    fd.write(chunk)
    return path

def save_to_cache(note_id: str, data: Dict[str, Any]) -> None:
    """
    å°†å°çº¢ä¹¦ç¬”è®°æ•°æ®ä¿å­˜åˆ°ç¼“å­˜
    
    :param note_id: ç¬”è®°ID
    :param data: ç¬”è®°æ•°æ®
    """
    cache_path = os.path.join(CACHE_DIR, f"{note_id}.json")
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        
        # æ·»åŠ ç¼“å­˜æ—¶é—´æˆ³
        cache_data = {
            "timestamp": int(time.time()),
            "data": data
        }
        
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å°çº¢ä¹¦æ•°æ®å·²ç¼“å­˜: {note_id}")
    except Exception as e:
        logger.error(f"ä¿å­˜å°çº¢ä¹¦ç¼“å­˜å¤±è´¥: {e}")

def get_from_cache(note_id: str, max_age: int = 86400) -> Optional[Dict[str, Any]]:
    """
    ä»ç¼“å­˜è·å–å°çº¢ä¹¦ç¬”è®°æ•°æ®
    
    :param note_id: ç¬”è®°ID
    :param max_age: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å¤©
    :return: ç¬”è®°æ•°æ®æˆ–None
    """
    cache_path = os.path.join(CACHE_DIR, f"{note_id}.json")
    
    if not os.path.exists(cache_path):
        return None
    
    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        timestamp = cache_data.get("timestamp", 0)
        current_time = int(time.time())
        
        if current_time - timestamp > max_age:
            logger.info(f"å°çº¢ä¹¦ç¼“å­˜å·²è¿‡æœŸ: {note_id}")
            return None
        
        return cache_data.get("data")
    except Exception as e:
        logger.error(f"è¯»å–å°çº¢ä¹¦ç¼“å­˜å¤±è´¥: {e}")
        return None

def extract_note_info(note_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»ç¬”è®°æ•°æ®ä¸­æå–å…³é”®ä¿¡æ¯
    
    :param note_data: åŸå§‹ç¬”è®°æ•°æ®
    :return: æå–åçš„å…³é”®ä¿¡æ¯
    """
    # ä»åŸå§‹æ•°æ®ä¸­æå–åŸºæœ¬ä¿¡æ¯
    liked = note_data.get('liked', False)
    like_count = note_data.get('likeCount', 0)
    collected = note_data.get('collected', False)
    collect_count = note_data.get('collectCount', 0)
    comment_count = note_data.get('commentCount', 0)
    share_count = note_data.get('shareCount', 0)
    
    # å°è¯•ä»shareInfoä¸­è·å–æ›´å¤šä¿¡æ¯
    share_info = note_data.get('shareInfo', {})
    note_id = share_info.get('noteId', '')
    note_type = share_info.get('type', 'normal')
    title = share_info.get('title', 'æ— æ ‡é¢˜')
    location = share_info.get('location', '')
    time_stamp = share_info.get('time', 0)
    
    # æå–ç”¨æˆ·ä¿¡æ¯
    user_info = share_info.get('user', {})
    user_id = user_info.get('userId', '')
    nickname = user_info.get('nickname', 'æœªçŸ¥ä½œè€…')
    avatar = user_info.get('avatar', '')
    
    # æå–å›¾ç‰‡åˆ—è¡¨ (æå–urlDefaultä½œä¸ºé«˜è´¨é‡å›¾ç‰‡é“¾æ¥)
    image_list = []
    for img in note_data.get('imageList', []):
        if 'urlDefault' in img and img['urlDefault']:
            image_list.append({
                'url': img['urlDefault'],
                'width': img.get('width', 0),
                'height': img.get('height', 0)
            })
    
    # æå–è§†é¢‘ä¿¡æ¯
    video_info = {}
    if note_type == 'video' and 'video' in note_data:
        video = note_data.get('video', {})
        video_info = {
            'url': video.get('url', ''),
            'cover': video.get('cover', {}).get('url', '')
        }
    
    # æ„å»ºç»“æœ
    result = {
        'note_id': note_id,
        'type': note_type,
        'title': title,
        'desc': note_data.get('desc', ''),
        'location': location,
        'time': time_stamp,
        'user': {
            'user_id': user_id,
            'nickname': nickname,
            'avatar': avatar
        },
        'stats': {
            'liked': liked,
            'like_count': like_count, 
            'collected': collected,
            'collect_count': collect_count,
            'comment_count': comment_count,
            'share_count': share_count
        },
        'images': image_list,
        'video': video_info
    }
    
    return result

async def process_xiaohongshu_url(event: AstrMessageEvent, xhs_ck: str) -> AsyncGenerator[Any, None]:
    """
    å¤„ç†å°çº¢ä¹¦é“¾æ¥
    
    :param event: æ¶ˆæ¯äº‹ä»¶
    :param xhs_ck: å°çº¢ä¹¦cookie
    :return: å¼‚æ­¥ç”Ÿæˆç»“æœ
    """
    if not xhs_ck:
        yield event.plain_result("æ— æ³•è·å–åˆ°å°çº¢ä¹¦Cookieï¼Œè¯·åœ¨é…ç½®ä¸­è®¾ç½®XHS_CK")
        return
    
    # æå–URL - åŒ¹é…å½¢å¦‚ https://www.xiaohongshu.com/explore/6841430e000000002300f126 çš„é“¾æ¥
    message_str = event.message_str.strip()
    msg_url_match = re.search(r"(https?:\/\/)?(?:www\.)?(xhslink\.com|xiaohongshu\.com)\/[A-Za-z\d._?%&+\-=\/#@]*", message_str)
    
    if not msg_url_match:
        return
    
    msg_url = msg_url_match.group(0)
    
    # è¯·æ±‚å¤´
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
        'cookie': xhs_ck,
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # å¦‚æœæ˜¯çŸ­é“¾æ¥ï¼Œè·å–å®Œæ•´é“¾æ¥
    if "xhslink" in msg_url:
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(msg_url, headers=headers, follow_redirects=True)
                msg_url = str(response.url)
        except Exception as e:
            yield event.plain_result(f"è§£æå°çº¢ä¹¦é“¾æ¥å¤±è´¥: {str(e)}")
            return
    
    # æå–å°çº¢ä¹¦ID
    xhs_id_match = re.search(r'/explore/(\w+)', msg_url)
    if not xhs_id_match:
        xhs_id_match = re.search(r'/discovery/item/(\w+)', msg_url)
    if not xhs_id_match:
        xhs_id_match = re.search(r'source=note&noteId=(\w+)', msg_url)
    
    if not xhs_id_match:
        yield event.plain_result(f"æ— æ³•ä»é“¾æ¥ä¸­æå–å°çº¢ä¹¦ID")
        return
    
    xhs_id = xhs_id_match.group(1)
    
    # é¦–å…ˆå°è¯•ä»ç¼“å­˜è·å–æ•°æ®
    cached_data = get_from_cache(xhs_id)
    note_data = None
    
    if cached_data:
        logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å°çº¢ä¹¦æ•°æ®: {xhs_id}")
        note_data = cached_data
    else:
        # è§£æURLå‚æ•°
        parsed_url = urlparse(msg_url)
        params = parse_qs(parsed_url.query)
        
        # æå–å‚æ•°
        xsec_source = params.get('xsec_source', ['pc_feed'])[0]
        xsec_token = params.get('xsec_token', [None])[0]
        
        # è¯·æ±‚å°çº¢ä¹¦å†…å®¹
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f'{XHS_REQ_LINK}{xhs_id}?xsec_source={xsec_source}&xsec_token={xsec_token}', 
                    headers=headers
                )
                html = response.text
        except Exception as e:
            yield event.plain_result(f"è¯·æ±‚å°çº¢ä¹¦å†…å®¹å¤±è´¥: {str(e)}")
            return
        
        # è§£æJSONæ•°æ®
        try:
            response_json_match = re.search('window.__INITIAL_STATE__=(.*?)</script>', html)
            if not response_json_match:
                yield event.plain_result("æ— æ³•è§£æå°çº¢ä¹¦å†…å®¹ï¼ŒCookieå¯èƒ½å·²å¤±æ•ˆ")
                return
                
            response_json_str = response_json_match.group(1).replace("undefined", "null")
            response_json = json.loads(response_json_str)
            
            raw_note_data = response_json['note']['noteDetailMap'][xhs_id]['note']
            
            # æå–å¹¶ç¼“å­˜æ•°æ®
            note_data = extract_note_info(raw_note_data)
            save_to_cache(xhs_id, note_data)
            
        except Exception as e:
            yield event.plain_result(f"è§£æå°çº¢ä¹¦å†…å®¹å¤±è´¥: {str(e)}")
            return
    
    if not note_data:
        yield event.plain_result("æ— æ³•è·å–å°çº¢ä¹¦å†…å®¹")
        return
        
    # æå–å¸–å­ä¿¡æ¯
    content_type = note_data.get('type', '')
    note_title = note_data.get('title', 'æ— æ ‡é¢˜')
    note_desc = note_data.get('desc', '')
    author_name = note_data.get('user', {}).get('nickname', 'æœªçŸ¥ä½œè€…')
    location = note_data.get('location', '')
    
    # æ„å»ºå›å¤å†…å®¹
    reply_text = f"å°çº¢ä¹¦è§£æ | {note_title}"
    if note_desc:
        reply_text += f"\næè¿°: {note_desc}"
    
    reply_text += f"\nä½œè€…: {author_name}"
    
    if location:
        reply_text += f"\nä½ç½®: {location}"
    
    stats = note_data.get('stats', {})
    liked_icon = "â¤ï¸" if stats.get('liked', False) else "ğŸ‘"
    collected_icon = "â­" if stats.get('collected', False) else "â­"
    
    reply_text += f"\n{liked_icon} {stats.get('like_count', 0)} | ğŸ’¬ {stats.get('comment_count', 0)} | {collected_icon} {stats.get('collect_count', 0)}"
    
    if 'time' in note_data and note_data['time']:
        # è½¬æ¢æ—¶é—´æˆ³ä¸ºå¯è¯»æ—¶é—´
        time_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(note_data['time']/1000))
        reply_text += f"\nå‘å¸ƒæ—¶é—´: {time_str}"
    
    # å‘é€åˆå§‹ä¿¡æ¯
    yield event.plain_result(reply_text)
    
    # æ ¹æ®å†…å®¹ç±»å‹å¤„ç†
    if content_type == 'normal':
        # å›¾ç‰‡å¸–å­
        image_list = note_data.get('images', [])
        if not image_list:
            yield event.plain_result("æœªæ‰¾åˆ°å›¾ç‰‡å†…å®¹")
            return
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        os.makedirs(TEMP_DIR, exist_ok=True)
        
        # ä¸‹è½½å›¾ç‰‡
        image_paths = []
        async with aiohttp.ClientSession() as session:
            download_tasks = []
            for index, item in enumerate(image_list):
                image_url = item.get('url', '')
                if not image_url:
                    continue
                path = os.path.join(TEMP_DIR, f"xhs_{xhs_id}_{index}.jpg")
                download_tasks.append(asyncio.create_task(
                    download_img(image_url, path, session=session)))
            
            if download_tasks:
                image_paths = await asyncio.gather(*download_tasks)
        
        if not image_paths:
            yield event.plain_result("å›¾ç‰‡ä¸‹è½½å¤±è´¥")
            return
        
        # åˆ›å»ºè½¬å‘æ¶ˆæ¯å†…å®¹
        content_list = []
        
        # æ·»åŠ æ ‡é¢˜å’Œæè¿°
        content_list.append([
            Comp.Plain(f"å°çº¢ä¹¦ | {note_title}\nä½œè€…: {author_name}")
        ])
        
        if note_desc:
            content_list.append([
                Comp.Plain(f"æè¿°: {note_desc}")
            ])
        
        # æ·»åŠ ä½ç½®ä¿¡æ¯
        if location:
            content_list.append([
                Comp.Plain(f"ä½ç½®: {location}")
            ])
            
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        content_list.append([
            Comp.Plain(f"{liked_icon} {stats.get('like_count', 0)} | ğŸ’¬ {stats.get('comment_count', 0)} | {collected_icon} {stats.get('collect_count', 0)}")
        ])
        
        # æ·»åŠ æ—¶é—´ä¿¡æ¯
        if 'time' in note_data and note_data['time']:
            time_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(note_data['time']/1000))
            content_list.append([
                Comp.Plain(f"å‘å¸ƒæ—¶é—´: {time_str}")
            ])
        
        # æ·»åŠ å›¾ç‰‡
        for i, path in enumerate(image_paths):
            content_list.append([
                Comp.Plain(f"ç¬¬ {i+1}/{len(image_paths)} å¼ "),
                Comp.Image.fromFileSystem(path)
            ])
        
        # å‘é€åˆå¹¶è½¬å‘æ¶ˆæ¯
        yield await send_forward_message(event, content_list)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        remove_files(image_paths)
        
    elif content_type == 'video':
        # è§†é¢‘å¸–å­
        video_info = note_data.get('video', {})
        video_url = video_info.get('url', '')
        
        if not video_url:
            yield event.plain_result("æ— æ³•è·å–è§†é¢‘é“¾æ¥")
            return
        
        # ä¸‹è½½è§†é¢‘
        try:
            video_path = await download_video(video_url)
            
            # å‘é€è§†é¢‘
            yield event.chain_result([
                Comp.Video.fromFileSystem(video_path)
            ])
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            remove_files([video_path])
        except Exception as e:
            yield event.plain_result(f"è§†é¢‘å¤„ç†å¤±è´¥: {str(e)}")
    else:
        yield event.plain_result(f"ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {content_type}") 